{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量拼接与切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 torch.cat() 将张量按维度dim进行拼接 params：\n",
    "\n",
    "tensors：张量序列\n",
    "\n",
    "dim：要拼接的维度\n",
    "\n",
    "1.2 torch.stack() 在新创建的维度dim上进行拼接 会扩张维度 params：\n",
    "\n",
    "tensors：张量序列\n",
    "\n",
    "dim：要拼接的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]]),\n",
       " torch.Size([8, 6]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " torch.Size([4, 12]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_0 = torch.cat([t,t],dim=0)\n",
    "t_1 = torch.cat([t,t],dim=1)\n",
    "t_0,t_0.shape,t_1,t_1.shape\n",
    "#不扩展维度 只是在原有矩阵的基础上增加长高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]]]),\n",
       " torch.Size([4, 6, 2]),\n",
       " tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]),\n",
       " torch.Size([4, 6, 3]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_3 = torch.stack([t,t],dim=2)\n",
    "t_4 = torch.stack([t,t,t],dim=2)\n",
    "t_3,t_3.shape,t_4,t_4.shape\n",
    "#会扩展维度 把两个二维矩阵拼接成三维 增加深度这一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]]]),\n",
       " torch.Size([3, 4, 6]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_s = torch.stack([t,t,t],dim=0)\n",
    "t_s,t_s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 torch.chunk()  将张量按维度dim平均切分 返回张量列表 若不能整除 最后一份张量小于其他张量\n",
    "\n",
    "params：\n",
    "\n",
    "input；要切分的张量\n",
    "\n",
    "chunks：要切分的份数\n",
    "\n",
    "dim：要切分的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]]),\n",
       " (tensor([[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]]),\n",
       "  tensor([[1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1.]])),\n",
       " (tensor([[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]),\n",
       "  tensor([[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]])),\n",
       " (tensor([[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]]),\n",
       "  tensor([[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]]),\n",
       "  tensor([[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c = torch.chunk(t,2,0)\n",
    "t_c1 = torch.chunk(t,2,1)\n",
    "t_c2 = torch.chunk(t,3,1)\n",
    "t,t_c,t_c1,t_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "第0个张量，维度为torch.Size([4, 3])\n",
      "第1个张量，维度为torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "for idx,t in enumerate(t_c1):\n",
    "    print('第{}个张量，维度为{}'.format(idx,t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 torch.split() 将张量按维度dim进行切分 返回张量列表 params：\n",
    "\n",
    "tensors:要切分的张量\n",
    "\n",
    "split_size_or_sections:为int时表示每一份的长度 为list时 按list元素切分\n",
    "\n",
    "dim：要切分的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "第1个张量 tensor([[1., 1., 1., 1., 1., 1.]]) 形状为torch.Size([1, 6])\n",
      "第2个张量 tensor([[1., 1., 1., 1., 1., 1.]]) 形状为torch.Size([1, 6])\n",
      "第3个张量 tensor([[1., 1., 1., 1., 1., 1.]]) 形状为torch.Size([1, 6])\n",
      "第4个张量 tensor([[1., 1., 1., 1., 1., 1.]]) 形状为torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "print(t.shape)\n",
    "list = torch.split(t,1,dim=0)\n",
    "for idx,t in enumerate(list):\n",
    "    print('第{}个张量 {} 形状为{}'.format(idx+1,t,t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "第1个张量 tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) 形状为torch.Size([2, 6])\n",
      "第2个张量 tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) 形状为torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "print(t.shape)\n",
    "list = torch.split(t,2,dim=0)\n",
    "for idx,t in enumerate(list):\n",
    "    print('第{}个张量 {} 形状为{}'.format(idx+1,t,t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "第1个张量 tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) 形状为torch.Size([4, 2])\n",
      "第2个张量 tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) 形状为torch.Size([4, 2])\n",
      "第3个张量 tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) 形状为torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "print(t.shape)\n",
    "list = torch.split(t,2,dim=1)\n",
    "for idx,t in enumerate(list):\n",
    "    print('第{}个张量 {} 形状为{}'.format(idx+1,t,t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "第1个张量 tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) 形状为torch.Size([4, 3])\n",
      "第2个张量 tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) 形状为torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "print(t.shape)\n",
    "list = torch.split(t,3,dim=1)\n",
    "for idx,t in enumerate(list):\n",
    "    print('第{}个张量 {} 形状为{}'.format(idx+1,t,t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "第1个张量 tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) 形状为torch.Size([4, 4])\n",
      "第2个张量 tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) 形状为torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "print(t.shape)\n",
    "list = torch.split(t,4,dim=1)\n",
    "for idx,t in enumerate(list):\n",
    "    print('第{}个张量 {} 形状为{}'.format(idx+1,t,t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "第1个张量 tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) 形状为torch.Size([4, 2])\n",
      "第2个张量 tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]]) 形状为torch.Size([4, 1])\n",
      "第3个张量 tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) 形状为torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((4,6))\n",
    "print(t.shape)\n",
    "list = torch.split(t,[2,1,3],dim=1)\n",
    "for idx,t in enumerate(list):\n",
    "    print('第{}个张量 {} 形状为{}'.format(idx+1,t,t.shape))\n",
    "    #list之和一定要和对应dim的长度相同 不然切不完会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 torch.index_select() 在维度dim上 按index索引数据 返回根据index索引数据拼接的张量 params：\n",
    "\n",
    "input：要索引的张量\n",
    "\n",
    "dim：要索引的维度\n",
    "\n",
    "index：要索引数据的序号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8, 4, 8],\n",
       "         [6, 7, 1],\n",
       "         [8, 8, 3]]),\n",
       " tensor([[8, 4, 8],\n",
       "         [8, 8, 3]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(0,9,size=(3,3))\n",
    "idx = torch.tensor([0,2],dtype=torch.long)\n",
    "t_select = torch.index_select(t,dim=0,index=idx)\n",
    "t,t_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8, 4, 8],\n",
       "         [6, 7, 1],\n",
       "         [8, 8, 3]]),\n",
       " tensor([[8, 8],\n",
       "         [6, 1],\n",
       "         [8, 3]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_select = torch.index_select(t,dim=1,index=idx)\n",
    "t,t_select\n",
    "#idx必须是torch.long类型的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 torch.masked_select() 按mask中的true进行索引 返回一维张量 params：\n",
    "\n",
    "input:要索引的张量  \n",
    "\n",
    "mask:与input形状相同的布尔类型张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 1, 3]), tensor([8, 8, 6, 7, 8, 8]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1 = t.le(5)\n",
    "mask2 = t.ge(5)\n",
    "#ge mean greater or equal le mean less or queal\n",
    "#生成的是一维的 因为不清楚mask中true的具体个数 所以一维张量返回所有的\n",
    "t1 = torch.masked_select(t,mask1)\n",
    "t2 = torch.masked_select(t,mask2)\n",
    "t1,t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 torch.reshape() 变换张量形状 当张量在内存中连续时 新张量与input共享内存数据 params：\n",
    "\n",
    "input：要变换的张量\n",
    "\n",
    "shape：新张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8, 4, 8],\n",
       "         [6, 7, 1],\n",
       "         [8, 8, 3]]),\n",
       " torch.Size([3, 3]),\n",
       " tensor([[8, 4, 8, 6, 7, 1, 8, 8, 3]]),\n",
       " torch.Size([1, 9]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.reshape(t,(1,-1))\n",
    "t,t.shape,t1,t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1024,    4,    8],\n",
       "         [   6,    7,    1],\n",
       "         [   8,    8,    3]]),\n",
       " tensor([[1024,    4,    8,    6,    7,    1,    8,    8,    3]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0,0] = 1024\n",
    "t,t1\n",
    "#共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 torch.transpose() 交换张量的两个维度 params：\n",
    "\n",
    "input:要交换的张量\n",
    "\n",
    "dim0：要交换的维度\n",
    "\n",
    "dim1：要交换的维度\n",
    "\n",
    "3.3 torch.t() 二维张量转置 对于矩阵而言 等价于 torch.transpose(input,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 4, 2, 8],\n",
       "         [5, 1, 2, 4]]),\n",
       " torch.Size([2, 4]),\n",
       " tensor([[1, 5],\n",
       "         [4, 1],\n",
       "         [2, 2],\n",
       "         [8, 4]]),\n",
       " torch.Size([4, 2]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(1,9,size=(2,4))\n",
    "t1 = torch.transpose(t,dim0=0,dim1=1)\n",
    "t,t.shape,t1,t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 5],\n",
       "         [4, 1],\n",
       "         [2, 2],\n",
       "         [8, 4]]),\n",
       " torch.Size([4, 2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.t(t)\n",
    "t2,t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.7564, 0.7267, 0.9752, 0.6333],\n",
       "          [0.3655, 0.7822, 0.9801, 0.9474],\n",
       "          [0.5055, 0.5060, 0.1989, 0.6586]],\n",
       " \n",
       "         [[0.6402, 0.1760, 0.0632, 0.9340],\n",
       "          [0.9895, 0.8237, 0.1121, 0.8900],\n",
       "          [0.6555, 0.7817, 0.7128, 0.2255]]]),\n",
       " torch.Size([2, 3, 4]),\n",
       " tensor([[[0.7564, 0.3655, 0.5055],\n",
       "          [0.7267, 0.7822, 0.5060],\n",
       "          [0.9752, 0.9801, 0.1989],\n",
       "          [0.6333, 0.9474, 0.6586]],\n",
       " \n",
       "         [[0.6402, 0.9895, 0.6555],\n",
       "          [0.1760, 0.8237, 0.7817],\n",
       "          [0.0632, 0.1121, 0.7128],\n",
       "          [0.9340, 0.8900, 0.2255]]]),\n",
       " torch.Size([2, 4, 3]),\n",
       " tensor([[[0.7564, 0.6402],\n",
       "          [0.3655, 0.9895],\n",
       "          [0.5055, 0.6555]],\n",
       " \n",
       "         [[0.7267, 0.1760],\n",
       "          [0.7822, 0.8237],\n",
       "          [0.5060, 0.7817]],\n",
       " \n",
       "         [[0.9752, 0.0632],\n",
       "          [0.9801, 0.1121],\n",
       "          [0.1989, 0.7128]],\n",
       " \n",
       "         [[0.6333, 0.9340],\n",
       "          [0.9474, 0.8900],\n",
       "          [0.6586, 0.2255]]]),\n",
       " torch.Size([4, 3, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((2,3,4))\n",
    "t1 = torch.transpose(t,1,2)\n",
    "t2 = torch.transpose(t,0,2)\n",
    "t,t.shape,t1,t1.shape,t2,t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像预处理 读进来C H W 要转换成 H W C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 torch.squeeze() 压缩长度为1的维度 params：\n",
    "\n",
    "dim：若为none 移除所有长度为1的轴 若指定维度 当且仅当该轴长度为1时可以被移除\n",
    "\n",
    "3.5 torch.unsqueeze() 依据dim拓展维度 params：\n",
    "\n",
    "dim：扩展的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.9578],\n",
       "           [0.2025],\n",
       "           [0.7577]],\n",
       " \n",
       "          [[0.1533],\n",
       "           [0.1176],\n",
       "           [0.1901]]]]),\n",
       " torch.Size([1, 2, 3, 1]),\n",
       " tensor([[0.9578, 0.2025, 0.7577],\n",
       "         [0.1533, 0.1176, 0.1901]]),\n",
       " torch.Size([2, 3]),\n",
       " tensor([[[0.9578],\n",
       "          [0.2025],\n",
       "          [0.7577]],\n",
       " \n",
       "         [[0.1533],\n",
       "          [0.1176],\n",
       "          [0.1901]]]),\n",
       " torch.Size([2, 3, 1]),\n",
       " tensor([[[[0.9578],\n",
       "           [0.2025],\n",
       "           [0.7577]],\n",
       " \n",
       "          [[0.1533],\n",
       "           [0.1176],\n",
       "           [0.1901]]]]),\n",
       " torch.Size([1, 2, 3, 1]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((1,2,3,1))\n",
    "t1 = torch.squeeze(t)\n",
    "t2 = torch.squeeze(t,dim=0)\n",
    "t3 = torch.squeeze(t,dim=1)\n",
    "t,t.shape,t1,t1.shape,t2,t2.shape,t3,t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量数学运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.add() 逐元素计算input+alpha*other params：\n",
    "\n",
    "input：第一个张量\n",
    "\n",
    "alpha：乘项因子\n",
    "\n",
    "other：第二个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18772\\AppData\\Local\\Temp\\ipykernel_3440\\2117869421.py:3: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1485.)\n",
      "  t_add = torch.add(t,10,t1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3507, -0.8734, -0.5810],\n",
       "         [ 0.1203, -1.4329,  0.0373],\n",
       "         [ 0.2603, -0.4851,  0.9874]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[11.3507,  9.1266,  9.4190],\n",
       "         [10.1203,  8.5671, 10.0373],\n",
       "         [10.2603,  9.5149, 10.9874]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn((3,3))\n",
    "t1 = torch.ones_like(t)\n",
    "t_add = torch.add(t,10,t1)\n",
    "t,t1,t_add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
